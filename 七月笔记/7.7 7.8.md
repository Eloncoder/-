# 目录
## makefile

## C++

##

# NVDLA Primer
## Abstract

The majority of compute effort for Deep Learning inference is based on mathematical operations that can mostly be grouped into four parts: convolutions; activations; pooling; and normalization.   
深度学习推理的大部分计算工作基于数学运算，这些运算主要可以分为四个部分：卷积； 激活； 池化； 和标准化。  
These operations share a few characteristics that make them particularly well suited for special-purpose hardware implementation: their memory access patterns are extremely predictable, and they are readily parallelized.   
这些操作有一些共同的特点，使它们特别适合特殊用途的硬件实现：它们的内存访问模式非常可预测，并且它们很容易并行化。  
The NVIDIA® Deep Learning Accelerator (NVDLA) project promotes a standardized, open architecture to address the computational demands of inference.   
NVIDIA® 深度学习加速器 (NVDLA) 项目推动标准化的开放式架构来满足推理的计算需求。  
The NVDLA architecture is both scalable and highly configurable; the modular design maintains flexibility and simplifies integration.   
NVDLA 架构具有可扩展性和高度可配置性； 模块化设计保持了灵活性并简化了集成方式。  
Standardizing Deep Learning acceleration promotes interoperability with the majority of modern Deep Learning networks and contributes to a unified growth of machine learning at scale.  
标准化深度学习加速可促进与大多数现代深度学习网络的互操作性，并有助于大规模机器学习的统一增长。  

NVDLA hardware provides a simple, flexible, robust inference acceleration solution.   
NVDLA 硬件提供了一种简单、灵活、稳健的推理加速解决方案。  
It supports a wide range of performance levels and readily scales for applications ranging from smaller, cost-sensitive Internet of Things (IoT) devices to larger performance oriented IoT devices.   
它支持广泛的性能水平的设备，并且可以轻松扩展应用程序，从小型、成本敏感的物联网 (IoT) 设备到面向高性能的大型物联网设备。  
NVDLA is provided as a set of IP-core models based on open industry standards: the Verilog model is a synthesis and simulation model in RTL form, and the TLM SystemC simulation model can be used for software development, system integration, and testing.   
NVDLA 是作为一组基于开放行业标准的 IP 核模型提供的：Verilog 模型是 RTL 形式的综合和仿真模型，TLM SystemC 仿真模型可用于软件开发、系统集成和测试。  
The NVDLA software ecosystem includes an on-device software stack (part of the open source release), a full training infrastructure to build new models that incorporate Deep Learning, and parsers that convert existing models to a form that is usable by the on-device software.  
NVDLA 软件生态系统包括设备端软件堆栈（开源版本的一部分）、用于构建包含深度学习的新模型的完整训练基础设施，以及将现有模型转换为设备端可用形式的解析器 软件。  

## NVDLA软件设计
NVDLA 拥有完整的软件生态系统来支持它。与 NVDLA 相关的软件分为两组：**编译工具（模型转换）** 和 **runtime环境（在 NVDLA 上加载和执行网络的run-time软件）**。这些的总体流程如下图所示；
![image](https://user-images.githubusercontent.com/63440757/177896845-0628d961-2677-4c64-8c77-ca83ff50cd5a.png)

### 编译工具：模型创建和编译
编译工具包括编译器和解析器。编译器负责创建一系列硬件层，这些层针对给定的 NVDLA 配置进行优化；拥有优化的硬件层网络通过减少模型大小、负载和运行时间来提高性能。编译是一个分门别类的多步骤过程，可分为两个基本组件：解析和编译。解析器可以相对简单；在其最基本的体现中，它可以读取预先训练的Caffe模型，并创建网络的"中间表示"，以传递到编译的下一步。编译器将 NVDLA 实现的解析中间表示和硬件配置作为输入，并生成硬件层网络。这些步骤是离线执行的，并可能在包含 NVDLA 实现的设备上执行。

了解 NVDLA 实现的特定硬件配置非常重要，它使编译器能够为可用的特征生成适当的层。例如，这可能包括在不同的卷积操作模式（如 Winograd 卷积或基本卷积）之间进行选择，或根据可用的卷积缓冲大小将卷积操作拆分为多个较小的小型操作。此阶段还负责量化模型以降低精度，如 8 位或 16 位整数或 16 位浮点，并分配用于权重的内存区域。同一编译工具可用于生成多个不同 NVDLA 配置的操作列表。

### Runtime环境：设备上模型推理
Runtime环境涉及在兼容的 NVDLA 硬件上运行模型。它实际上分为两层：

用户模式驱动（UMD）。用户模式程序的主要接口。分析神经网络后，编译器逐层编译网络，并将其转换为称为 “NVDLA Loadable” 的文件格式。用户模式runtime驱动加载此loadable，并将推理工作提交给“内核模式驱动”。
内核模式驱动（KMD）。由驱动和固件组成，这些驱动和固件可执行 NVDLA 上调度层操作的工作，并编程 NVDLA 寄存器以配置每个功能块。
Runtime执行从存储的网络表示开始；此存储格式称为 “NVDLA loadable” 图像。在loadable的视图中，NVDLA 实现中的每个功能块都由软件中的 “层” 表示；每一层都包含有关其依赖性的信息、它用作输入和输出的内存中的张量以及每个块用于操作的特定配置。层通过依赖性图连接在一起，内核模式驱动（KMD）用于安排每次操作。NVDLA loadable格式在编译器实现和用户模式驱动（UMD）实现中实现标准化。所有符合 NVDLA 标准的实现至少应该能够理解任何 NVDLA loadable图像，即使实现可能没有使用该loadable图像运行推理所需的某些特征。

UMD 具有标准的应用程序编程接口 （API），用于处理可加载图像、将输入和输出张量绑定到内存位置以及运行推理。此层以一组定义的数据结构将网络加载到内存中，并以实现定义的方式将其传递给 KMD。例如，在 Linux 上，这可能是一个ioctl()，将数据从用户模式驱动传递到内核模式驱动；在 KMD 运行在与 UMD 相同环境的单一处理系统中，这可能是一个简单的函数调用。

KMD 的主要切入点在内存中接收推理工作，从多个可用工作中选择用于调度（如果在多处理系统上），并将其提交给核心引擎调度器。此核心引擎调度器负责处理 NVDLA 中断、每个函数块上的调度层以及根据上一层任务的完成更新该层的任何依赖关系。调度程序使用依赖性图中的信息来确定后续层何时准备被调度；这允许编译器以优化的方式决定层的调度，并避免不同的KMD实现的性能差异。
![image](https://user-images.githubusercontent.com/63440757/177896986-a52f01ee-135b-4464-aa67-0ba9cd4b71e2.png)

UMD 堆栈和 KMD 堆栈都以定义的 API 存在，并且预计将用系统可移植性层包裹。在可移植性层中保持核心实现预计需要的更改相对较少，并在可能需要多个平台上运行 NVDLA 软件堆栈时加快任何努力；在适当的可移植性层到位后，在 Linux 和 FreeRTOS 上应容易编译相同的核心实现。同样，在具有与 NVDLA 紧密耦合的微控制器的 “headed” 实现上，便携式层的存在使得在微控制器上与在没有此类配套处理器的 “headless” 实现中在主 CPU 上运行相同的低级软件成为可能。
