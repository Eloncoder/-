# 目录
## makefile

## C++

##

# NVDLA Primer
## Abstract

The majority of compute effort for Deep Learning inference is based on mathematical operations that can mostly be grouped into four parts: convolutions; activations; pooling; and normalization.   
深度学习推理的大部分计算工作基于数学运算，这些运算主要可以分为四个部分：卷积； 激活； 池化； 和标准化。  
These operations share a few characteristics that make them particularly well suited for special-purpose hardware implementation: their memory access patterns are extremely predictable, and they are readily parallelized.   
这些操作有一些共同的特点，使它们特别适合特殊用途的硬件实现：它们的内存访问模式非常可预测，并且它们很容易并行化。  
The NVIDIA® Deep Learning Accelerator (NVDLA) project promotes a standardized, open architecture to address the computational demands of inference.   
NVIDIA® 深度学习加速器 (NVDLA) 项目推动标准化的开放式架构来满足推理的计算需求。  
The NVDLA architecture is both scalable and highly configurable; the modular design maintains flexibility and simplifies integration.   
NVDLA 架构具有可扩展性和高度可配置性； 模块化设计保持了灵活性并简化了集成方式。  
Standardizing Deep Learning acceleration promotes interoperability with the majority of modern Deep Learning networks and contributes to a unified growth of machine learning at scale.  
标准化深度学习加速可促进与大多数现代深度学习网络的互操作性，并有助于大规模机器学习的统一增长。  

NVDLA hardware provides a simple, flexible, robust inference acceleration solution.   
NVDLA 硬件提供了一种简单、灵活、稳健的推理加速解决方案。  
It supports a wide range of performance levels and readily scales for applications ranging from smaller, cost-sensitive Internet of Things (IoT) devices to larger performance oriented IoT devices.   
它支持广泛的性能水平的设备，并且可以轻松扩展应用程序，从小型、成本敏感的物联网 (IoT) 设备到面向高性能的大型物联网设备。  
NVDLA is provided as a set of IP-core models based on open industry standards: the Verilog model is a synthesis and simulation model in RTL form, and the TLM SystemC simulation model can be used for software development, system integration, and testing.   
NVDLA 是作为一组基于开放行业标准的 IP 核模型提供的：Verilog 模型是 RTL 形式的综合和仿真模型，TLM SystemC 仿真模型可用于软件开发、系统集成和测试。  
The NVDLA software ecosystem includes an on-device software stack (part of the open source release), a full training infrastructure to build new models that incorporate Deep Learning, and parsers that convert existing models to a form that is usable by the on-device software.  
NVDLA 软件生态系统包括设备端软件堆栈（开源版本的一部分）、用于构建包含深度学习的新模型的完整训练基础设施，以及将现有模型转换为设备端可用形式的解析器 软件。  

## NVDLA软件设计
NVDLA 拥有完整的软件生态系统来支持它。与 NVDLA 相关的软件分为两组：**编译工具（模型转换）** 和 **runtime环境（在 NVDLA 上加载和执行网络的run-time软件）**。这些的总体流程如下图所示；
![image](https://user-images.githubusercontent.com/63440757/177896845-0628d961-2677-4c64-8c77-ca83ff50cd5a.png)

### Loadable文件
NVDLA的软件栈主要分为两个部分:Compiler与Runtime，由于Compiler与硬件无关，所以可以在我们自己的开发机器上编译运行调试，理解起来也较为方便；而Runtime与硬件有关，调试非常困难，**Loadable文件是两者之间通信的媒介**


### 编译工具：模型创建和编译
编译工具包括编译器和解析器。编译器负责创建一系列硬件层，这些层针对给定的 NVDLA 配置进行优化；拥有优化的硬件层网络通过减少模型大小、负载和运行时间来提高性能。编译是一个分门别类的多步骤过程，可分为两个基本组件：解析和编译。解析器可以相对简单；在其最基本的体现中，它可以读取预先训练的Caffe模型，并创建网络的"中间表示"，以传递到编译的下一步。编译器将 NVDLA 实现的解析中间表示和硬件配置作为输入，并生成硬件层网络。这些步骤是离线执行的，并可能在包含 NVDLA 实现的设备上执行。

了解 NVDLA 实现的特定硬件配置非常重要，它使编译器能够为可用的特征生成适当的层。例如，这可能包括在不同的卷积操作模式（如 Winograd 卷积或基本卷积）之间进行选择，或根据可用的卷积缓冲大小将卷积操作拆分为多个较小的小型操作。此阶段还负责量化模型以降低精度，如 8 位或 16 位整数或 16 位浮点，并分配用于权重的内存区域。同一编译工具可用于生成多个不同 NVDLA 配置的操作列表。

### Runtime环境：设备上模型推理
Runtime环境涉及在兼容的 NVDLA 硬件上运行模型。它实际上分为两层：

**用户模式驱动（UMD）**。用户模式程序的主要接口。分析神经网络后，编译器逐层编译网络，并将其转换为称为 “NVDLA Loadable” 的文件格式。用户模式runtime驱动加载此loadable，并将推理工作提交给“内核模式驱动”。

**内核模式驱动（KMD）**。由驱动和固件组成，这些驱动和固件可执行 NVDLA 上调度层操作的工作，并编程 NVDLA 寄存器以配置每个功能块。
Runtime执行从存储的网络表示开始；此存储格式称为 “NVDLA loadable” 图像。在loadable的视图中，NVDLA 实现中的每个功能块都由软件中的 “层” 表示；每一层都包含有关其依赖性的信息、它用作输入和输出的内存中的张量以及每个块用于操作的特定配置。层通过依赖性图连接在一起，内核模式驱动（KMD）用于安排每次操作。NVDLA loadable格式在编译器实现和用户模式驱动（UMD）实现中实现标准化。所有符合 NVDLA 标准的实现至少应该能够理解任何 NVDLA loadable图像，即使实现可能没有使用该loadable图像运行推理所需的某些特征。

UMD 具有标准的应用程序编程接口 （API），用于处理可加载图像、将输入和输出张量绑定到内存位置以及运行推理。此层以一组定义的数据结构将网络加载到内存中，并以实现定义的方式将其传递给 KMD。例如，在 Linux 上，这可能是一个ioctl()，将数据从用户模式驱动传递到内核模式驱动；在 KMD 运行在与 UMD 相同环境的单一处理系统中，这可能是一个简单的函数调用。

KMD 的主要切入点在内存中接收推理工作，从多个可用工作中选择用于调度（如果在多处理系统上），并将其提交给核心引擎调度器。此核心引擎调度器负责处理 NVDLA 中断、每个函数块上的调度层以及根据上一层任务的完成更新该层的任何依赖关系。调度程序使用依赖性图中的信息来确定后续层何时准备被调度；这允许编译器以优化的方式决定层的调度，并避免不同的KMD实现的性能差异。
![image](https://user-images.githubusercontent.com/63440757/177896986-a52f01ee-135b-4464-aa67-0ba9cd4b71e2.png)

UMD 堆栈和 KMD 堆栈都以定义的 API 存在，并且预计将用系统可移植性层包裹。在可移植性层中保持核心实现预计需要的更改相对较少，并在可能需要多个平台上运行 NVDLA 软件堆栈时加快任何努力；在适当的可移植性层到位后，在 Linux 和 FreeRTOS 上应容易编译相同的核心实现。同样，在具有与 NVDLA 紧密耦合的微控制器的 “headed” 实现上，便携式层的存在使得在微控制器上与在没有此类配套处理器的 “headless” 实现中在主 CPU 上运行相同的低级软件成为可能。

参考：https://blog.csdn.net/qq_41019681/article/details/118544546

## TensorRT
TensorRT是可以在**NVIDIA**各种**GPU硬件平台**下运行的一个**C++推理框架**。我们利用Pytorch、TF或者其他框架训练好的模型，可以转化为TensorRT的格式，然后利用TensorRT推理引擎去运行我们这个模型，从而提升这个模型在英伟达GPU上运行的速度。速度提升的比例是比较可观的。

TensorRT是由C++、CUDA、python三种语言编写成的一个库，其中核心代码为**C++和CUDA**，**Python**端作为前端与用户交互。当然，TensorRT也是支持C++前端的，如果我们追求高性能，C++前端调用TensorRT是必不可少的。

加速效果取决于模型的类型和大小，也取决于我们所使用的显卡类型。
对于GPU来说，因为底层的硬件设计，更适合**并行计算**也更喜欢**密集型计算**。TensorRT所做的优化也是基于GPU进行优化，当然也是更喜欢那种一大块一大块的矩阵运算，尽量直通到底。因此对于通道数比较多的卷积层和反卷积层，优化力度是比较大的；如果是比较繁多复杂的各种**细小op**操作(例如reshape、gather、split等)，那么TensorRT的优化力度就没有那么夸张了。

reshape：改变指定形状
gather：获取指定索引的值
split：将张量分割成子张量

为什么TensorRT能够提升我们模型在英伟达GPU上运行的速度，当然是做了很多对**提速有增益**的优化：
![image](https://user-images.githubusercontent.com/63440757/177986437-01a2fe67-68da-4e6f-90cd-63b5942d5f14.png)

算子融合(层与张量融合)：简单来说就是通过**融合**一些计算op或者**去掉**一些多余op来减少数据流通次数以及显存的频繁使用来提速  
量化：量化即**IN8量化**或者**FP16**以及**TF32**等不同于常规FP32精度的使用，这些精度可以显著提升模型执行速度并且会保持原先模型的精度  
内核自动调整：根据不同的显卡构架、SM数量、内核频率等(例如1080TI和2080TI)，选择**不同的优化策略以及计算方式**，寻找最合适当前构架的计算方式  
动态张量显存：我们都知道，**显存的开辟和释放**是比较耗时的，通过调整一些策略可以减少模型中这些操作的次数，从而可以减少模型运行的时间  
多流执行：使用CUDA中的**stream技术**，最大化实现并行操作   

![image](https://user-images.githubusercontent.com/63440757/177987136-ca3f37ae-610c-453f-b0e6-081e73b420a3.png)

左上角是原始网络(googlenet)，右上角相对原始层进行了垂直优化，将conv+bias(BN)+relu进行了融合优化；而右下角进行了水平优化，将所有1x1的CBR融合成一个大的CBR；左下角则将concat层直接去掉，将contact层的输入直接送入下面的操作中，不用单独进行concat后在输入计算，相当于减少了一次传输吞吐；

